---
title: "Figure of ttr and text length"
author: "Yawen Yu"
date: "3/29/2018"
output:
  html_document: default
  pdf_document: default
---
#### Install packages
```{r, message=F, warning=F}
library(dplyr)
library(tidyverse)
library(tidytext)
library(feather)
library(koRpus)
library(childesr)
```

* Load speech data
```{r, eval=FALSE, echo=FALSE, message=F}
# get utterances 
utter <- get_utterances(collection = c("Eng-NA","Eng-UK"), corpus = NULL) 

# get children's id
childes_intercept <- read_feather("/Users/Yawen/Desktop/lexical diversity/trial5_ldp/childes_intercept.feather")
kid_id <- unique(childes_intercept$subject)

# remove unintelligible utterances
murmur = c("xxx", "yyy", "yyy_yyy","---")

utter_kid <- utter %>%
  filter(speaker_code == "CHI")%>%
  mutate(stem = removeWords(stem, murmur),
         stem = gsub("[^[:alnum:]]", " ", stem),
         stem = tolower(stem)) %>%
  filter(!grepl("^\\s*$", stem)) %>%
  filter(corpus_id == corpus_id) %>%
  filter(speaker_id == speaker_id) %>%
  filter(target_child_age == target_child_age) %>%
  group_by(corpus_id, speaker_id, target_child_age) %>%
  summarise(utts = paste0(stem, collapse = " "))%>%
  mutate(age = floor(target_child_age/30.5)) %>%
  ungroup(.)%>%
  select(-target_child_age)%>%
  rename(corpus = corpus_id, 
         subject = speaker_id) %>%
  filter(subject %in% kid_id)

# unnest words
word <- utter_kid %>%
  filter(subject %in% kid_id)%>%
  unnest_tokens(word, utts) %>%
  rename(token = word) 
```

* Sample words
```{r,message = F, eval=F, echo=FALSE}
# sample tokens successevily of each children
all_word <- data.frame()

for(i in kid_id){
      
    word_subset <- word %>%
        filter(subject == i) 
    
  for(j in 1:300){
      
    word_sample <- data.frame(subject = i,
                              utts = head(word_subset$token, n = j)) %>%
    summarise(utts = paste0(utts, collapse = " ")) %>%
      mutate(subject = i,
             length = j)
 
    all_word <- all_word %>%
      rbind(word_sample)
  }
}


# tokenize sampled texts
all_token <- all_word %>%
    split(paste0(.$subject, "-",.$length, "-")) 


# create functions to measure Lexical diversity
ld_fun <- function(df) {
  tokenized_df <- koRpus::tokenize(df$utts, format = "obj",lang = "en", tag = TRUE) #tokenize texts
  MTLD <- MTLD(tokenized_df)
  TTR <- TTR(tokenized_df)# measure LD
  MATTR <- MATTR(tokenized_df)
  HDD <- HDD(tokenized_df)
  value <- data_frame(mtld = MTLD@MTLD$MTLD,
                      mattr = MATTR@MATTR$MATTR,
                      vocd = HDD@HDD$HDD,
                      ttr = TTR@TTR) # get LD measurement
}

# measure LD
sample_ld <- map(all_token, ld_fun) %>% 
  bind_rows(.id = "length") %>%
  separate(col = length, into = c("subject","length"), sep = "-") %>%
  filter(!mtld == "Inf",
         !is.na(mattr))%>%
  mutate(length = as.numeric(length),
         subject = as.numeric(subject),
         age = "20mon",
         mtld = scale(mtld),
         mattr = scale(mattr),
         ttr = scale(ttr),
         vocd = scale(vocd))%>%
  gather(measure, value, 
         mtld, mattr, vocd, ttr)


write_feather(sample_ld,"/Users/Yawen/Desktop/lexical diversity/trial6_childes/sample_ld.feather" )
```

#### Plot
```{r, message=F, warning=F}
sample_ld <- read_feather("/Users/Yawen/Desktop/lexical diversity/trial6_childes/sample_ld.feather")

sample_ld %>%
  ggplot(., aes(x = length, y = value, color = measure))+
  geom_smooth(se=F)+
  facet_grid(~measure)+
  theme_classic()+
  labs(title = "Points of Stabilization")


# plot 3 sampled children
set.seed(100)
sample_id <- sample(x=unique(sample_ld$subject), 3)

sample_ld %>%
  filter(subject %in% sample_id)%>%
  ggplot(., aes(x = length, y = value, color=measure))+
  geom_point()+
  geom_smooth(se=F)+
  facet_grid(~measure)+
  theme_classic()+
  labs(title = "Points of Stabilization",
       subtitle = "Three Sampled children")
```
